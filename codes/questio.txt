10. Employee Bonus Calculation Based on Performance and Department
Classify employees for a bonus eligibility program. Employees in "Sales" and "Marketing" with
performance scores above 80 get a 20% bonus, while others with scores above 70 get 15%. All other
employees receive no bonus. Group by department and calculate total bonus allocation.
 Scala Spark Data
val employees = List(
("karthik", "Sales", 85),
("neha", "Marketing", 78),
("priya", "IT", 90),
("mohan", "Finance", 65),
("ajay", "Sales", 55),
("vijay", "Marketing", 82),
("veer", "HR", 72),
("aatish", "Sales", 88),
("animesh", "Finance", 95),
("nishad", "IT", 60)
).toDF("name", "department", "performance_score")
 PySpark Data
employees = [
("karthik", "Sales", 85),
("neha", "Marketing", 78),
("priya", "IT", 90),
("mohan", "Finance", 65),
("ajay", "Sales", 55),
("vijay", "Marketing", 82),
("veer", "HR", 72),
("aatish", "Sales", 88),
("animesh", "Finance", 95),
("nishad", "IT", 60)
]
employees_df = spark.createDataFrame(employees, ["name", "department", "performance_score"])
 Expected Output: Grouped by department, showing names, bonus percentage, and total
bonus amount by department.
11. Product Return Analysis with Multi-Level Classification
For each product, classify return reasons as "High Return Rate" if return count exceeds 100 and
satisfaction score below 50, "Moderate Return Rate" if return count is between 50-100 with a score
between 50-70, and "Low Return Rate" otherwise. Group by category to count product return rates.
 Scala Spark Data
val products = List(
("Laptop", "Electronics", 120, 45),
("Smartphone", "Electronics", 80, 60),
("Tablet", "Electronics", 50, 72),
("Headphones", "Accessories", 110, 47),
("Shoes", "Clothing", 90, 55),
("Jacket", "Clothing", 30, 80),
("TV", "Electronics", 150, 40),
("Watch", "Accessories", 60, 65),
("Pants", "Clothing", 25, 75),
("Camera", "Electronics", 95, 58)
).toDF("product_name", "category", "return_count", "satisfaction_score")
 PySpark Data
products = [
("Laptop", "Electronics", 120, 45),
("Smartphone", "Electronics", 80, 60),
("Tablet", "Electronics", 50, 72),
("Headphones", "Accessories", 110, 47),
("Shoes", "Clothing", 90, 55),
("Jacket", "Clothing", 30, 80),
("TV", "Electronics", 150, 40),
("Watch", "Accessories", 60, 65),
("Pants", "Clothing", 25, 75),
("Camera", "Electronics", 95, 58)
]
products_df = spark.createDataFrame(products, ["product_name", "category", "return_count",
"satisfaction_score"])
 Expected Output: Shows the product name, return rate classification, and total number of
products in each return rate category for each category.
12. Customer Spending Pattern Based on Age and Membership Level
Classify customers' spending as "High Spender" if spending exceeds $1000 with "Premium"
membership, "Average Spender" if spending between $500-$1000 and membership is "Standard",
and "Low Spender" otherwise. Group by membership and calculate average spending.
 Scala Spark Data
val customers = List(
("karthik", "Premium", 1050, 32),
("neha", "Standard", 800, 28),
("priya", "Premium", 1200, 40),
("mohan", "Basic", 300, 35),
("ajay", "Standard", 700, 25),
("vijay", "Premium", 500, 45),
("veer", "Basic", 450, 33),
("aatish", "Standard", 600, 29),
("animesh", "Premium", 1500, 60),
("nishad", "Basic", 200, 21)
).toDF("name", "membership", "spending", "age")
 PySpark Data
customers = [
("karthik", "Premium", 1050, 32),
("neha", "Standard", 800, 28),
("priya", "Premium", 1200, 40),
("mohan", "Basic", 300, 35),
("ajay", "Standard", 700, 25),
("vijay", "Premium", 500, 45),
("veer", "Basic", 450, 33),
("aatish", "Standard", 600, 29),
("animesh", "Premium", 1500, 60),
("nishad", "Basic", 200, 21)
]
customers_df = spark.createDataFrame(customers, ["name", "membership", "spending", "age"])
 Expected Output: Displays customers' names, spending category, and average spending by
membership type.
13. E-commerce Order Fulfillment Timeliness Based on Product Type and Location
Classify orders as "Delayed" if delivery time exceeds 7 days and origin location is "International",
"On-Time" if between 3-7 days, and "Fast" if below 3 days. Group by product type to see the count of
each delivery speed category.
 Scala Spark Data
val orders = List(
("Order1", "Laptop", "Domestic", 2),
("Order2", "Shoes", "International", 8),
("Order3", "Smartphone", "Domestic", 3),
("Order4", "Tablet", "International", 5),
("Order5", "Watch", "Domestic", 7),
("Order6", "Headphones", "International", 10),
("Order7", "Camera", "Domestic", 1),
("Order8", "Shoes", "International", 9),
("Order9", "Laptop", "Domestic", 6),
("Order10", "Tablet", "International", 4)
).toDF("order_id", "product_type", "origin", "delivery_days")
 PySpark Data
orders = [
("Order1", "Laptop", "Domestic", 2),
("Order2", "Shoes", "International", 8),
("Order3", "Smartphone", "Domestic", 3),
("Order4", "Tablet", "International", 5),
("Order5", "Watch", "Domestic", 7),
("Order6", "Headphones", "International", 10),
("Order7", "Camera", "Domestic", 1),
("Order8", "Shoes", "International", 9),
("Order9", "Laptop", "Domestic", 6),
("Order10", "Tablet", "International", 4)
]
orders_df = spark.createDataFrame(orders, ["order_id", "product_type", "origin", "delivery_days"])
 Expected Output: Orders categorized by delivery speed, showing the number of each type
per product.
Scenario 14: Financial Risk Level Classification for Loan Applicants
Question Set:
1. Classify loan applicants as "High Risk" if the loan amount exceeds twice their income and
credit score is below 600, "Moderate Risk" if the loan amount is between 1-2 times their
income and credit score between 600-700, and "Low Risk" otherwise. Find the total count of
each risk level.
2. For applicants classified as "High Risk," calculate the average loan amount by income range
(e.g., < 50k, 50-100k, >100k).
3. Group by income brackets (<50k, 50-100k, >100k) and calculate the average credit score for
each risk level. Filter for groups where average credit score is below 650.
 Scala Spark Data
val loanApplicants = List(
("karthik", 60000, 120000, 590),
("neha", 90000, 180000, 610),
("priya", 50000, 75000, 680),
("mohan", 120000, 240000, 560),
("ajay", 45000, 60000, 620),
("vijay", 100000, 100000, 700),
("veer", 30000, 90000, 580),
("aatish", 85000, 85000, 710),
("animesh", 50000, 100000, 650),
("nishad", 75000, 200000, 540)
).toDF("name", "income", "loan_amount", "credit_score")
 PySpark Data
loan_applicants = [
("karthik", 60000, 120000, 590),
("neha", 90000, 180000, 610),
("priya", 50000, 75000, 680),
("mohan", 120000, 240000, 560),
("ajay", 45000, 60000, 620),
("vijay", 100000, 100000, 700),
("veer", 30000, 90000, 580),
("aatish", 85000, 85000, 710),
("animesh", 50000, 100000, 650),
("nishad", 75000, 200000, 540)
]
loan_applicants_df = spark.createDataFrame(loan_applicants, ["name", "income", "loan_amount",
"credit_score"])
Scenario 15: Customer Purchase Recency Categorization
Question Set: 4. Categorize customers based on purchase recency: "Frequent" if last purchase within
30 days, "Occasional" if within 60 days, and "Rare" if over 60 days. Show the number of each
category per membership type.
5. Find the average total purchase amount for customers with "Frequent" purchase recency
and "Premium" membership.
6. For customers with "Rare" recency, calculate the minimum purchase amount across different
membership types.
 Scala Spark Data
val customerPurchases = List(
("karthik", "Premium", 50, 5000),
("neha", "Standard", 10, 2000),
("priya", "Premium", 65, 8000),
("mohan", "Basic", 90, 1200),
("ajay", "Standard", 25, 3500),
("vijay", "Premium", 15, 7000),
("veer", "Basic", 75, 1500),
("aatish", "Standard", 45, 3000),
("animesh", "Premium", 20, 9000),
("nishad", "Basic", 80, 1100)
).toDF("name", "membership", "days_since_last_purchase", "total_purchase_amount")
 PySpark Data
customer_purchases = [
("karthik", "Premium", 50, 5000),
("neha", "Standard", 10, 2000),
("priya", "Premium", 65, 8000),
("mohan", "Basic", 90, 1200),
("ajay", "Standard", 25, 3500),
("vijay", "Premium", 15, 7000),
("veer", "Basic", 75, 1500),
("aatish", "Standard", 45, 3000),
("animesh", "Premium", 20, 9000),
("nishad", "Basic", 80, 1100)
]
customer_purchases_df = spark.createDataFrame(customer_purchases, ["name", "membership",
"days_since_last_purchase", "total_purchase_amount"])
Scenario 16: Electricity Consumption and Rate Assignment
Question Set: 7. Classify households into "High Usage" if kWh exceeds 500 and bill exceeds $200,
"Medium Usage" for kWh between 200-500 and bill between $100-$200, and "Low Usage"
otherwise. Calculate the total number of households in each usage category.
8. Find the maximum bill amount for "High Usage" households and calculate the average kWh
for "Medium Usage" households.
9. Identify households with "Low Usage" but kWh usage exceeding 300. Count such
households.
 Scala Spark Data
val electricityUsage = List(
("House1", 550, 250),
("House2", 400, 180),
("House3", 150, 50),
("House4", 500, 200),
("House5", 600, 220),
("House6", 350, 120),
("House7", 100, 30),
("House8", 480, 190),
("House9", 220, 105),
("House10", 150, 60)
).toDF("household", "kwh_usage", "total_bill")
 PySpark Data
electricity_usage = [
("House1", 550, 250),
("House2", 400, 180),
("House3", 150, 50),
("House4", 500, 200),
("House5", 600, 220),
("House6", 350, 120),
("House7", 100, 30),
("House8", 480, 190),
("House9", 220, 105),
("House10", 150, 60)
]
electricity_usage_df = spark.createDataFrame(electricity_usage, ["household", "kwh_usage",
"total_bill"])
Scenario 17: Employee Salary Band and Performance Classification
Question Set: 10. Classify employees into salary bands: "Senior" if salary > 100k and experience > 10
years, "Mid-level" if salary between 50-100k and experience 5-10 years, and "Junior" otherwise.
Group by department to find count of each salary band.
11. For each salary band, calculate the average performance score. Filter for bands where
average performance exceeds 80.
12. Find employees in "Mid-level" band with performance above 85 and experience over 7 years.
 Scala Spark Data
val employees = List(
("karthik", "IT", 110000, 12, 88),
("neha", "Finance", 75000, 8, 70),
("priya", "IT", 50000, 5, 65),
("mohan", "HR", 120000, 15, 92),
("ajay", "IT", 45000, 3, 50),
("vijay", "Finance", 80000, 7, 78),
("veer", "Marketing", 95000, 6, 85),
("aatish", "HR", 100000, 9, 82),
("animesh", "Finance", 105000, 11, 88),
("nishad", "IT", 30000, 2, 55)
).toDF("name", "department", "salary", "experience", "performance_score")
 PySpark Data
employees = [
("karthik", "IT", 110000, 12, 88),
("neha", "Finance", 75000, 8, 70),
("priya", "IT", 50000, 5, 65),
("mohan", "HR", 120000, 15, 92),
("ajay", "IT", 45000, 3, 50),
("vijay", "Finance", 80000,
Scenario 18: Product Sales Analysis
Question Set:
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
1. Classify products as "Top Seller" if total sales exceed 200,000 and discount offered is less
than 10%, "Moderate Seller" if total sales are between 100,000 and 200,000, and "Low
Seller" otherwise. Count the total number of products in each classification.
2. Find the maximum sales value among "Top Seller" products and the minimum discount rate
among "Moderate Seller" products.
3. Identify products from the "Low Seller" category with a total sales value below 50,000 and
discount offered above 15%.
 Scala Spark Data
val productSales = List(
("Product1", 250000, 5),
("Product2", 150000, 8),
("Product3", 50000, 20),
("Product4", 120000, 10),
("Product5", 300000, 7),
("Product6", 60000, 18),
("Product7", 180000, 9),
("Product8", 45000, 25),
("Product9", 70000, 15),
("Product10", 10000, 30)
).toDF("product_name", "total_sales", "discount")
 PySpark Data
product_sales = [
("Product1", 250000, 5),
("Product2", 150000, 8),
("Product3", 50000, 20),
("Product4", 120000, 10),
("Product5", 300000, 7),
("Product6", 60000, 18),
("Product7", 180000, 9),
("Product8", 45000, 25),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Product9", 70000, 15),
("Product10", 10000, 30)
]
product_sales_df = spark.createDataFrame(product_sales, ["product_name", "total_sales",
"discount"])
Scenario 19: Customer Loyalty Analysis
Question Set: 4. Classify customers as "Highly Loyal" if purchase frequency is greater than 20 times
and average spending is above 500, "Moderately Loyal" if frequency is between 10-20 times, and
"Low Loyalty" otherwise. Count customers in each classification.
5. Calculate the average spending of "Highly Loyal" customers and the minimum spending for
"Moderately Loyal" customers.
6. Identify "Low Loyalty" customers with an average spending less than 100 and purchase
frequency under 5.
 Scala Spark Data
val customerLoyalty = List(
("Customer1", 25, 700),
("Customer2", 15, 400),
("Customer3", 5, 50),
("Customer4", 18, 450),
("Customer5", 22, 600),
("Customer6", 2, 80),
("Customer7", 12, 300),
("Customer8", 6, 150),
("Customer9", 10, 200),
("Customer10", 1, 90)
).toDF("customer_name", "purchase_frequency", "average_spending")
 PySpark Data
customer_loyalty = [
("Customer1", 25, 700),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Customer2", 15, 400),
("Customer3", 5, 50),
("Customer4", 18, 450),
("Customer5", 22, 600),
("Customer6", 2, 80),
("Customer7", 12, 300),
("Customer8", 6, 150),
("Customer9", 10, 200),
("Customer10", 1, 90)
]
customer_loyalty_df = spark.createDataFrame(customer_loyalty, ["customer_name",
"purchase_frequency", "average_spending"])
Scenario 20: E-commerce Return Rate Analysis
Question Set: 7. Classify products by return rate: "High Return" if return rate is over 20%, "Medium
Return" if return rate is between 10% and 20%, and "Low Return" otherwise. Count products in each
classification.
8. Calculate the average sale price for "High Return" products and the maximum return rate for
"Medium Return" products.
9. Identify "Low Return" products with a sale price under 50 and return rate less than 5%.
 Scala Spark Data
val ecommerceReturn = List(
("Product1", 75, 25),
("Product2", 40, 15),
("Product3", 30, 5),
("Product4", 60, 18),
("Product5", 100, 30),
("Product6", 45, 10),
("Product7", 80, 22),
("Product8", 35, 8),
("Product9", 25, 3),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Product10", 90, 12)
).toDF("product_name", "sale_price", "return_rate")
 PySpark Data
ecommerce_return = [
("Product1", 75, 25),
("Product2", 40, 15),
("Product3", 30, 5),
("Product4", 60, 18),
("Product5", 100, 30),
("Product6", 45, 10),
("Product7", 80, 22),
("Product8", 35, 8),
("Product9", 25, 3),
("Product10", 90, 12)
]
ecommerce_return_df = spark.createDataFrame(ecommerce_return, ["product_name", "sale_price",
"return_rate"])
Scenario 21: Employee Productivity Scoring
Question Set: 10. Classify employees as "High Performer" if productivity score > 80 and project count
is greater than 5, "Average Performer" if productivity score is between 60 and 80, and "Low
Performer" otherwise. Count employees in each classification.
11. Calculate the average productivity score for "High Performer" employees and the minimum
score for "Average Performers."
12. Identify "Low Performers" with a productivity score below 50 and project count under 2.
 Scala Spark Data
val employeeProductivity = List(
("Emp1", 85, 6),
("Emp2", 75, 4),
("Emp3", 40, 1),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Emp4", 78, 5),
("Emp5", 90, 7),
("Emp6", 55, 3),
("Emp7", 80, 5),
("Emp8", 42, 2),
("Emp9", 30, 1),
("Emp10", 68, 4)
).toDF("employee_id", "productivity_score", "project_count")
 PySpark Data
employee_productivity = [
("Emp1", 85, 6),
("Emp2", 75, 4),
("Emp3", 40, 1),
("Emp4", 78, 5),
("Emp5", 90, 7),
("Emp6", 55, 3),
("Emp7", 80, 5),
("Emp8", 42, 2),
("Emp9", 30, 1),
("Emp10", 68, 4)
]
employee_productivity_df = spark.createDataFrame(employee_productivity, ["employee_id",
"productivity_score", "project_count"])
Scenario 22: Banking Fraud Detection
Question Set:
1. Classify transactions as "High Risk" if the transaction amount is above 10,000 and frequency
of transactions from the same account within a day exceeds 5, "Moderate Risk" if the
amount is between 5,000 and 10,000 and frequency is between 2 and 5, and "Low Risk"
otherwise. Calculate the total number of transactions in each risk level.
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
2. Identify accounts with at least one "High Risk" transaction and the total amount transacted
by those accounts.
3. Find all "Moderate Risk" transactions where the account type is "Savings" and the amount is
above 7,500.
 Scala Spark Data
val transactions = List(
("Account1", "2024-11-01", 12000, 6, "Savings"),
("Account2", "2024-11-01", 8000, 3, "Current"),
("Account3", "2024-11-02", 2000, 1, "Savings"),
("Account4", "2024-11-02", 15000, 7, "Savings"),
("Account5", "2024-11-03", 9000, 4, "Current"),
("Account6", "2024-11-03", 3000, 1, "Current"),
("Account7", "2024-11-04", 13000, 5, "Savings"),
("Account8", "2024-11-04", 6000, 2, "Current"),
("Account9", "2024-11-05", 20000, 8, "Savings"),
("Account10", "2024-11-05", 7000, 3, "Savings")
).toDF("account_id", "transaction_date", "amount", "frequency", "account_type")
 PySpark Data
transactions = [
("Account1", "2024-11-01", 12000, 6, "Savings"),
("Account2", "2024-11-01", 8000, 3, "Current"),
("Account3", "2024-11-02", 2000, 1, "Savings"),
("Account4", "2024-11-02", 15000, 7, "Savings"),
("Account5", "2024-11-03", 9000, 4, "Current"),
("Account6", "2024-11-03", 3000, 1, "Current"),
("Account7", "2024-11-04", 13000, 5, "Savings"),
("Account8", "2024-11-04", 6000, 2, "Current"),
("Account9", "2024-11-05", 20000, 8, "Savings"),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Account10", "2024-11-05", 7000, 3, "Savings")
]
transactions_df = spark.createDataFrame(transactions, ["account_id", "transaction_date", "amount",
"frequency", "account_type"])
Scenario 23: Hospital Patient Readmission Analysis
Question Set: 4. Classify patients as "High Readmission Risk" if their last readmission interval (in
days) is less than 15 and their age is above 60, "Moderate Risk" if the interval is between 15 and 30
days, and "Low Risk" otherwise. Count patients in each category.
5. Find the average readmission interval for "High Readmission Risk" patients.
6. Identify "Moderate Risk" patients who were admitted to the "ICU" more than twice in the
past year.
 Scala Spark Data
val patients = List(
("Patient1", 62, 10, 3, "ICU"),
("Patient2", 45, 25, 1, "General"),
("Patient3", 70, 8, 2, "ICU"),
("Patient4", 55, 18, 3, "ICU"),
("Patient5", 65, 30, 1, "General"),
("Patient6", 80, 12, 4, "ICU"),
("Patient7", 50, 40, 1, "General"),
("Patient8", 78, 15, 2, "ICU"),
("Patient9", 40, 35, 1, "General"),
("Patient10", 73, 14, 3, "ICU")
).toDF("patient_id", "age", "readmission_interval", "icu_admissions", "admission_type")
 PySpark Data
patients = [
("Patient1", 62, 10, 3, "ICU"),
("Patient2", 45, 25, 1, "General"),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Patient3", 70, 8, 2, "ICU"),
("Patient4", 55, 18, 3, "ICU"),
("Patient5", 65, 30, 1, "General"),
("Patient6", 80, 12, 4, "ICU"),
("Patient7", 50, 40, 1, "General"),
("Patient8", 78, 15, 2, "ICU"),
("Patient9", 40, 35, 1, "General"),
("Patient10", 73, 14, 3, "ICU")
]
patients_df = spark.createDataFrame(patients, ["patient_id", "age", "readmission_interval",
"icu_admissions", "admission_type"])
Scenario 24: Student Graduation Prediction
Question Set: 7. Classify students as "At-Risk" if attendance is below 75% and the average test score
is below 50, "Moderate Risk" if attendance is between 75% and 85%, and "Low Risk" otherwise.
Calculate the number of students in each risk category.
8. Find the average score for students in the "At-Risk" category.
9. Identify "Moderate Risk" students who have scored above 70 in at least three subjects.
 Scala Spark Data
val students = List(
("Student1", 70, 45, 60, 65, 75),
("Student2", 80, 55, 58, 62, 67),
("Student3", 65, 30, 45, 70, 55),
("Student4", 90, 85, 80, 78, 76),
("Student5", 72, 40, 50, 48, 52),
("Student6", 88, 60, 72, 70, 68),
("Student7", 74, 48, 62, 66, 70),
("Student8", 82, 56, 64, 60, 66),
("Student9", 78, 50, 48, 58, 55),
Seekho Bigdata Institute
© 2024 Seekho Bigdata Institute. All rights reserved. www.seekhobigdata.com 9989454737
("Student10", 68, 35, 42, 52, 45)
).toDF("student_id", "attendance_percentage", "math_score", "science_score", "english_score",
"history_score")
 PySpark Data
students = [
("Student1", 70, 45, 60, 65, 75),
("Student2", 80, 55, 58, 62, 67),
("Student3", 65, 30, 45, 70, 55),
("Student4", 90, 85, 80, 78, 76),
("Student5", 72, 40, 50, 48, 52),
("Student6", 88, 60, 72, 70, 68),
("Student7", 74, 48, 62, 66, 70),
("Student8", 82, 56, 64, 60, 66),
("Student9", 78, 50, 48, 58, 55),
("Student10", 68, 35, 42, 52, 45)
]
students_df = spark.createDataFrame(students, ["student_id", "attendance_percentage",
"math_score", "science_score", "english_score", "history_score"])
